{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FPO-DYS","text":""},{"location":"#fpo-dys","title":"FPO-DYS","text":"<p> Daniel McKenzie, Samy Wu Fung, and Howard Heaton</p> <p>Summary</p> <p>Operator splitting can be used to design easy-to-train models for predict-and-optimize tasks, which scale effortlessly to problems with thousands of variables.</p> <p> Contact Us </p> <p>Key Steps</p> <ul> <li> Split polytope constraints into nonnegativity and affine constraints</li> <li> Evaluate model using three-operator splitting (with projections onto two constraint sets) for forward prop</li> <li> Backprop using JFB</li> </ul> <p> Preprint  Code  </p> <p>Abstract</p> <p>In many practical settings, a combinatorial problem must be repeatedly solved with similar, but distinct parameters. Yet, the parameters \\(w\\) are not directly observed; only contextual data \\(d\\) that correlates with \\(w\\) is available. It is tempting to use a neural network to predict \\(w\\) given \\(d\\), but training such a model requires reconciling the discrete nature of combinatorial optimization with the gradient-based frameworks used to train neural networks. One approach to overcoming this issue is to consider a continuous relaxation of the combinatorial problem.  While existing such approaches have shown to be highly effective on small problems (10--100 variables) they do not scale well to large problems. In this work, we show how recent results in operator splitting can be used to design such a system which is easy to train  and scales effortlessly to problems with thousands of variables.</p> <p>Citation</p> <pre><code>@article{mckenzie2023faster,\n         title={{Faster Predict-and-Optimize with Davis-Yin Splitting}},\n         author={McKenzie, Daniel and Wu Fung, Samy and Heaton, Howard},\n         journal={arXiv preprint arXiv:2301.13395},\n         year={2023}\n}\n</code></pre>"},{"location":"dys-net/","title":"DYS-Net","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract implementation of a Davis-Yin Splitting (DYS) layer in a neural network.</p> Note <p>The singular value decomposition of the matrix \\(\\mathsf{A}\\) is used for the projection onto the subspace of all \\(\\mathsf{x}\\) such that \\(\\mathsf{Ax=b}\\).</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>tensor</code> <p>Matrix for linear system</p> required <code>b</code> <code>tensor</code> <p>Measurement vector for linear system</p> required <code>device</code> <code>string</code> <p>Device on which to perform computations</p> <code>'mps'</code> <code>alpha</code> <code>float</code> <p>Step size for DYS updates</p> <code>0.05</code> Source code in <code>src/dys_opt_net.py</code> <pre><code>class DYS_opt_net(nn.Module, ABC):\n    ''' Abstract implementation of a Davis-Yin Splitting (DYS) layer in a neural network.\n\n        Note:\n            The singular value decomposition of the matrix $\\mathsf{A}$ is used for the\n            projection onto the subspace of all $\\mathsf{x}$ such that $\\mathsf{Ax=b}$.\n\n        Args:\n            A (tensor):      Matrix for linear system\n            b (tensor):      Measurement vector for linear system\n            device (string): Device on which to perform computations\n            alpha (float):   Step size for DYS updates\n\n    '''\n    def __init__(self, A, b, device='mps', alpha=0.05):\n        super().__init__()\n        self.device = device\n        self.b = b\n        self.alpha = alpha*torch.ones(1, device=self.device)\n        self.A = A\n        self.n1 = A.shape[0]  # Number of rows of A\n        self.n2 = A.shape[1]  # Number of columns of A\n\n        U, s, VT = torch.linalg.svd(self.A, full_matrices=False)\n        self.s_inv = torch.tensor([1/sing if sing &gt;=1e-6 else 0 for sing in s]).to(self.device)\n        self.V = torch.t(VT).to(self.device)\n        self.UT = torch.t(U).to(self.device)\n\n    def _project_C1(self, x):\n        ''' Projection to the non-negative orthant.\n\n        Args:\n            x (tensor): point in Euclidean space\n\n        Returns:\n            Px (tensor): projection of $\\mathsf{x}$ onto nonnegative orthant\n\n        '''\n        Px = torch.clamp(x, min=0)\n        return Px\n\n    def _project_C2(self, z):\n      ''' Projection to the subspace of all $\\mathsf{x}$ such that $\\mathsf{Ax=b}$.\n\n        Note:\n            The singular value decomposition (SVD) representation\n            of the matrix $\\mathsf{A}$ is used to efficiently compute\n            the projection.\n\n        Args:\n            z (tensor): point in Euclidean space\n\n        Returns:\n            Pz (tensor): projection onto subspace $\\mathsf{\\{z : Ax = b\\}}$\n\n      '''\n      res = self.A.matmul(z.permute(1,0)) - self.b.view(-1,1)\n      temp = self.V.matmul(self.s_inv.view(-1,1)*self.UT.matmul(res)).permute(1,0)\n      Pz = z - temp\n      return Pz\n\n    @abstractmethod\n    def F(self, z, w):\n        ''' Gradient of objective function. Must be defined for each problem type.\n\n            Note:\n                The parameters of $\\mathsf{F}$ are stored in $\\mathsf{w}$.\n\n            Args:\n                z (tensor): point in Euclidean space\n                w (tensor): Parameters defining objective function.\n\n            Returns:\n                Fz (tensor): Gradient of objective function at $\\mathsf{z}$\n        '''\n        pass\n\n    @abstractmethod\n    def _data_space_forward(self, d):\n      ''' Specify the map from context d to parameters of F. This will be used during training.\n\n            Args:\n                d (tensor): Contextual data\n\n            Returns:\n                w (tensor): inference/solution to parameterized optimization problem.\n      '''\n      pass\n\n    @abstractmethod\n    def test_time_forward(self, d):\n       '''\n       Specify test time behaviour, e.g. use a combinatorial solver on the forward pass.\n\n         Args:\n                d (tensor): Contextual data\n\n        Returns:\n                x (tensor): inference/solution to parameterized optimization problem.\n       '''\n       pass\n\n\n    def _apply_DYS(self, z, w): \n        ''' Apply a single update step from Davis-Yin Splitting. \n\n            Args:\n                z (tensor): Point in Euclidean space\n                w (tensor): Parameters defining function and its gradient\n\n            Returns:\n                z (tensor): Updated estimate of solution\n        '''\n        x = self._project_C1(z)\n        y = self._project_C2(2.0 * x - z - self.alpha*self.F(z, w))\n        z = z - x + y\n        return z\n\n\n    def _train_time_forward(self, d, eps=1.0e-2, max_depth=int(1e4), \n                depth_warning=True): \n        ''' Default forward behaviour during training.\n\n            Args:\n                d (tensor):           Contextual data\n                eps (float);          Stopping criterion threshold\n                max_depth (int):      Maximum number of DYS updates\n                depth_warning (bool): Boolean for whether to print warning message when max depth reached\n\n            Returns:\n                z (tensor): P+O Inference\n        '''\n        with torch.no_grad():\n            w = self._data_space_forward(d)\n            self.depth = 0.0\n\n            z = torch.rand((self.n2), device=self.device)\n            z_prev = z.clone()      \n\n            all_samp_conv = False\n            while not all_samp_conv and self.depth &lt; max_depth:\n                z_prev = z.clone()   \n                z = self._apply_DYS(z, w)\n                diff_norm = torch.norm(z - z_prev) \n                diff_norm = torch.norm( diff_norm) \n                diff_norm = torch.max( diff_norm ) # take norm along the latter two dimensions then max\n                self.depth += 1.0\n                all_samp_conv = diff_norm &lt;= eps\n\n        if self.depth &gt;= max_depth and depth_warning:\n            print(\"\\nWarning: Max Depth Reached - Break Forward Loop\\n\")\n        if self.training:\n            w = self._data_space_forward(d)\n            z = self._apply_DYS(z.detach(), w)\n            return self._project_C1(z)\n        else:\n            return self._project_C1(z).detach()\n\n\n    def forward(self, d, eps=1.0e-2, max_depth=int(1e4),\n                depth_warning=True):\n        ''' Forward propagation of DYS-net.\n\n            Note:\n                A switch is included for using different behaviour at test/deployment. \n\n            Args:\n                d (tensor):           Contextual data\n                eps (float);          Stopping criterion threshold\n                max_depth (int):      Maximum number of DYS updates\n                depth_warning (bool): Boolean for whether to print warning message when max depth reached\n\n            Returns:\n                z (tensor): P+O Inference        \n        '''\n        if not self.training:\n          return self.test_time_forward(d)\n\n        return self._train_time_forward(d, eps, max_depth, depth_warning)\n</code></pre>"},{"location":"dys-net/#src.dys_opt_net.DYS_opt_net.F","title":"<code>F(z, w)</code>  <code>abstractmethod</code>","text":"<p>Gradient of objective function. Must be defined for each problem type.</p> Note <p>The parameters of \\(\\mathsf{F}\\) are stored in \\(\\mathsf{w}\\).</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>tensor</code> <p>point in Euclidean space</p> required <code>w</code> <code>tensor</code> <p>Parameters defining objective function.</p> required <p>Returns:</p> Name Type Description <code>Fz</code> <code>tensor</code> <p>Gradient of objective function at \\(\\mathsf{z}\\)</p> Source code in <code>src/dys_opt_net.py</code> <pre><code>@abstractmethod\ndef F(self, z, w):\n    ''' Gradient of objective function. Must be defined for each problem type.\n\n        Note:\n            The parameters of $\\mathsf{F}$ are stored in $\\mathsf{w}$.\n\n        Args:\n            z (tensor): point in Euclidean space\n            w (tensor): Parameters defining objective function.\n\n        Returns:\n            Fz (tensor): Gradient of objective function at $\\mathsf{z}$\n    '''\n    pass\n</code></pre>"},{"location":"dys-net/#src.dys_opt_net.DYS_opt_net.forward","title":"<code>forward(d, eps=0.01, max_depth=int(10000.0), depth_warning=True)</code>","text":"<p>Forward propagation of DYS-net.</p> Note <p>A switch is included for using different behaviour at test/deployment. </p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>tensor</code> <p>Contextual data</p> required <code>max_depth</code> <code>int</code> <p>Maximum number of DYS updates</p> <code>int(10000.0)</code> <code>depth_warning</code> <code>bool</code> <p>Boolean for whether to print warning message when max depth reached</p> <code>True</code> <p>Returns:</p> Name Type Description <code>z</code> <code>tensor</code> <p>P+O Inference</p> Source code in <code>src/dys_opt_net.py</code> <pre><code>def forward(self, d, eps=1.0e-2, max_depth=int(1e4),\n            depth_warning=True):\n    ''' Forward propagation of DYS-net.\n\n        Note:\n            A switch is included for using different behaviour at test/deployment. \n\n        Args:\n            d (tensor):           Contextual data\n            eps (float);          Stopping criterion threshold\n            max_depth (int):      Maximum number of DYS updates\n            depth_warning (bool): Boolean for whether to print warning message when max depth reached\n\n        Returns:\n            z (tensor): P+O Inference        \n    '''\n    if not self.training:\n      return self.test_time_forward(d)\n\n    return self._train_time_forward(d, eps, max_depth, depth_warning)\n</code></pre>"},{"location":"dys-net/#src.dys_opt_net.DYS_opt_net.test_time_forward","title":"<code>test_time_forward(d)</code>  <code>abstractmethod</code>","text":"<p>Specify test time behaviour, e.g. use a combinatorial solver on the forward pass.</p> <p>Args:          d (tensor): Contextual data</p> <p>Returns:          x (tensor): inference/solution to parameterized optimization problem.</p> Source code in <code>src/dys_opt_net.py</code> <pre><code>@abstractmethod\ndef test_time_forward(self, d):\n   '''\n   Specify test time behaviour, e.g. use a combinatorial solver on the forward pass.\n\n     Args:\n            d (tensor): Contextual data\n\n    Returns:\n            x (tensor): inference/solution to parameterized optimization problem.\n   '''\n   pass\n</code></pre>"}]}