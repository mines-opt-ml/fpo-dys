{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-Sklearn cannot be imported.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x105d40550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "import pyepo\n",
    "from pyepo.model.grb import optGrbModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import time as time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from ModelsKnapSack import KnapSackNet, ValPredictNet \n",
    "\n",
    "# Set the random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "num_data = 1000 # number of data\n",
    "num_feat = 5 # size of feature\n",
    "num_item = 10 # number of items\n",
    "num_constraints = 2 # number of knapsacks\n",
    "weights_numpy, contexts_numpy, costs_numpy = pyepo.data.knapsack.genData(num_data, num_feat, num_item,\n",
    "                                                dim=num_constraints, deg=1, noise_width=0.5, seed=135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'\n",
    "weights = torch.Tensor(weights_numpy).to(device)\n",
    "# print(weights)\n",
    "## NB: Weights here refer to the weights of the items in the knapsack problem.\n",
    "# They become part of the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmckenzie/My-Drive/Research/Fixed_Point_Networks/Diff-Opt-Over-Polytopes-Project/SPO_with_DYS/Knapsack_Testing/dYS_opt_net.py:29: UserWarning: The operator 'aten::linalg_svd' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  U, s, VT = torch.linalg.svd(self.A, full_matrices=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KnapSackNet(\n",
       "  (fc_1): Linear(in_features=5, out_features=50, bias=True)\n",
       "  (fc_2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (fc_3): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capacities_numpy = 20*torch.ones(2)\n",
    "capacities = capacities_numpy.to(device)\n",
    "capacities_DYS = 18*torch.ones(2, device=device)\n",
    "DYS_net = KnapSackNet(weights, capacities_DYS, num_constraints, num_item, num_feat, device=device)\n",
    "DYS_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = DYS_net(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up the Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "d_train, d_test, w_train, w_test = train_test_split(contexts_numpy, costs_numpy, test_size=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2024-10-28\n"
     ]
    }
   ],
   "source": [
    "caps = capacities.cpu() #[20] * 2 # capacity\n",
    "optmodel = pyepo.model.grb.knapsackModel(weights_numpy, caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for optDataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 900/900 [00:00<00:00, 1427.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for optDataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 100/100 [00:00<00:00, 981.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# get optDataset\n",
    "dataset_train = pyepo.data.dataset.optDataset(optmodel, d_train, w_train)\n",
    "dataset_test = pyepo.data.dataset.optDataset(optmodel, d_test, w_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data loader\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 256\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set-up\n",
    "Hyperparameters, initialize optimizer, initialize arrays etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_hist = []\n",
    "fmt = '[{:4d}/{:4d}]: train loss = {:7.3e} | test_loss = {:7.3e} ]'\n",
    "train_start_time = time.time()\n",
    "epoch = 0\n",
    "max_epochs = 10\n",
    "train_loss_ave = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Testing Regret loss function\n",
    "# Make eval mode that rounds before computing cost.\n",
    "class RegretLoss(nn.Module):\n",
    "    def __init__(self, num_item, num_constraints, device):\n",
    "        super(RegretLoss, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_items = num_item  # number of items, i.e. non-dummy variables\n",
    "        self.num_constraints = num_constraints # number of constraints, i.e. dummy variables\n",
    "    \n",
    "    def forward(self, w_true, x_pred, x_opt, opt_value, eval_mode=False):\n",
    "        '''\n",
    "          d is (batch of) contexts, w_true is (batch of) true \n",
    "          cost vectors.\n",
    "        '''\n",
    "        if eval_mode:\n",
    "            regret = opt_value - (w_true*torch.round(x_pred[:,:-(self.num_constraints + self.num_items)])).sum(dim=-1)\n",
    "            regret = torch.mean(regret)/torch.mean(opt_value) #normalized regret\n",
    "        else:\n",
    "            regret = opt_value - (w_true*x_pred[:,:-(self.num_constraints + self.num_items)]).sum(dim=-1)\n",
    "            regret = torch.mean(regret)\n",
    "        return regret\n",
    "    \n",
    "    # cost = torch.matmul(w_true.view(-1, self.num_items), x_pred[:,:-self.num_constraints]) # leave out dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(DYS_net.parameters(), lr=1e-3, weight_decay = 5e-4)\n",
    "criterion = RegretLoss(num_item, num_constraints, device=device)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 test loss is  0.03321428596973419\n",
      "epoch:  1 test loss is  0.046071428805589676\n",
      "epoch:  2 test loss is  0.03428571671247482\n",
      "epoch:  3 test loss is  0.030714284628629684\n",
      "epoch:  4 test loss is  0.0446428582072258\n",
      "epoch:  5 test loss is  0.05535714328289032\n",
      "epoch:  6 test loss is  0.04357143118977547\n",
      "epoch:  7 test loss is  0.055714286863803864\n",
      "epoch:  8 test loss is  0.082857146859169\n",
      "epoch:  9 test loss is  0.05285714566707611\n",
      "epoch:  10 test loss is  0.08500000089406967\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "while epoch <= max_epochs:\n",
    "    for d_batch, w_batch, opt_sol, opt_value in loader_train:\n",
    "        d_batch = d_batch.to(device)\n",
    "        w_batch = w_batch.to(device)\n",
    "        opt_sol = opt_sol.to(device)\n",
    "        opt_value = opt_value.to(device)\n",
    "        DYS_net.train()\n",
    "        optimizer.zero_grad()\n",
    "        predicted = DYS_net(d_batch)\n",
    "        loss = criterion(w_batch, predicted, opt_sol, opt_value)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_ave = 0.95*train_loss_ave + 0.05*loss.item()\n",
    "        \n",
    "    DYS_net.eval()\n",
    "    test_loss = 0\n",
    "    for d_batch, w_batch, opt_sol, opt_value in loader_test:\n",
    "        d_batch = d_batch.to(device)\n",
    "        w_batch = w_batch.to(device)\n",
    "        opt_sol = opt_sol.to(device)\n",
    "        opt_value = opt_value.to(device)\n",
    "        predicted = DYS_net(d_batch)\n",
    "        test_loss += criterion(w_batch, predicted, opt_sol, opt_value, eval_mode=True).item()\n",
    "    \n",
    "    # print(predicted)\n",
    "    scheduler.step(test_loss)\n",
    "    test_loss_hist.append(test_loss)\n",
    "    print('epoch: ', epoch, 'test loss is ', test_loss)\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22., 25., 39., 20., 40., 25., 30., 26., 33., 27., 23., 29., 22., 27.,\n",
      "        28., 25., 29., 29., 25., 16., 26., 19., 29., 31., 30., 22., 29., 28.,\n",
      "        28., 35., 31., 25., 25., 15., 22., 29., 27., 21., 24., 26., 22., 37.,\n",
      "        24., 20., 23., 26., 32., 29., 32., 28., 26., 20., 16., 29., 22., 29.,\n",
      "        18., 26., 20., 22., 31., 25., 18., 32., 22., 25., 25., 20., 24., 23.,\n",
      "        21., 25., 33., 21., 26., 29., 25., 20., 29., 22., 22., 30., 15., 30.,\n",
      "        22., 24., 28., 21., 18., 21., 27., 21., 17., 25., 28., 29., 36., 26.,\n",
      "        30., 33.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print((w_batch*torch.round(predicted[:,:-(num_item + num_constraints)])).sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31.],\n",
      "        [30.],\n",
      "        [39.],\n",
      "        [26.],\n",
      "        [37.],\n",
      "        [27.],\n",
      "        [32.],\n",
      "        [29.],\n",
      "        [30.],\n",
      "        [27.],\n",
      "        [30.],\n",
      "        [29.],\n",
      "        [26.],\n",
      "        [27.],\n",
      "        [30.],\n",
      "        [29.],\n",
      "        [24.],\n",
      "        [29.],\n",
      "        [29.],\n",
      "        [23.],\n",
      "        [33.],\n",
      "        [22.],\n",
      "        [29.],\n",
      "        [31.],\n",
      "        [28.],\n",
      "        [26.],\n",
      "        [26.],\n",
      "        [29.],\n",
      "        [28.],\n",
      "        [31.],\n",
      "        [36.],\n",
      "        [28.],\n",
      "        [26.],\n",
      "        [19.],\n",
      "        [22.],\n",
      "        [28.],\n",
      "        [29.],\n",
      "        [29.],\n",
      "        [26.],\n",
      "        [26.],\n",
      "        [30.],\n",
      "        [37.],\n",
      "        [36.],\n",
      "        [21.],\n",
      "        [23.],\n",
      "        [30.],\n",
      "        [27.],\n",
      "        [38.],\n",
      "        [32.],\n",
      "        [31.],\n",
      "        [26.],\n",
      "        [26.],\n",
      "        [24.],\n",
      "        [29.],\n",
      "        [25.],\n",
      "        [27.],\n",
      "        [20.],\n",
      "        [26.],\n",
      "        [26.],\n",
      "        [25.],\n",
      "        [37.],\n",
      "        [26.],\n",
      "        [23.],\n",
      "        [32.],\n",
      "        [29.],\n",
      "        [23.],\n",
      "        [29.],\n",
      "        [26.],\n",
      "        [24.],\n",
      "        [26.],\n",
      "        [27.],\n",
      "        [26.],\n",
      "        [27.],\n",
      "        [27.],\n",
      "        [31.],\n",
      "        [33.],\n",
      "        [33.],\n",
      "        [21.],\n",
      "        [31.],\n",
      "        [22.],\n",
      "        [25.],\n",
      "        [34.],\n",
      "        [18.],\n",
      "        [30.],\n",
      "        [24.],\n",
      "        [26.],\n",
      "        [30.],\n",
      "        [25.],\n",
      "        [20.],\n",
      "        [32.],\n",
      "        [31.],\n",
      "        [25.],\n",
      "        [25.],\n",
      "        [28.],\n",
      "        [30.],\n",
      "        [27.],\n",
      "        [30.],\n",
      "        [30.],\n",
      "        [31.],\n",
      "        [31.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(opt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17.6200, 17.6200, 17.6200, 17.6200, 25.2100, 17.6200, 17.6200, 17.6200,\n",
      "         20.3700, 17.6200, 17.6200, 17.6200, 17.6200, 17.6200, 17.6200, 17.6200,\n",
      "         25.2100, 17.6200, 20.3700, 17.6200, 17.6200, 17.6200, 17.6200, 17.6200,\n",
      "         25.2100, 20.3700, 25.2100, 25.2100, 17.6200, 25.2100, 17.6200, 17.6200,\n",
      "         17.6200, 17.6200, 17.6200, 25.2100, 17.6200, 17.6200, 17.6200, 17.6200,\n",
      "         17.6200, 17.6200, 17.6200, 25.2100, 17.6200, 20.3700, 25.2100, 17.6200,\n",
      "         17.6200, 20.3700, 17.6200, 17.6200, 17.6200, 17.6200, 17.6200, 25.2100,\n",
      "         17.6200, 17.6200, 17.6200, 25.2100, 17.6200, 20.3700, 17.6200, 17.6200,\n",
      "         17.6200, 25.2100, 17.6200, 17.6200, 17.6200, 17.6200, 17.6200, 17.6200,\n",
      "         25.2100, 17.6200, 17.6200, 17.6200, 20.3700, 17.6200, 17.6200, 17.6200,\n",
      "         17.6200, 17.6200, 17.6200, 17.6200, 25.2100, 20.3700, 25.2100, 17.6200,\n",
      "         17.6200, 17.6200, 17.6200, 17.6200, 17.6200, 17.6200, 17.6200, 25.2100,\n",
      "         23.9900, 17.6200, 20.3700, 25.2100],\n",
      "        [19.6900, 19.6900, 19.6900, 19.6900, 22.9200, 19.6900, 19.6900, 19.6900,\n",
      "         16.8100, 19.6900, 19.6900, 19.6900, 19.6900, 19.6900, 19.6900, 19.6900,\n",
      "         22.9200, 19.6900, 16.8100, 19.6900, 19.6900, 19.6900, 19.6900, 19.6900,\n",
      "         22.9200, 16.8100, 22.9200, 22.9200, 19.6900, 22.9200, 19.6900, 19.6900,\n",
      "         19.6900, 19.6900, 19.6900, 22.9200, 19.6900, 19.6900, 19.6900, 19.6900,\n",
      "         19.6900, 19.6900, 19.6900, 22.9200, 19.6900, 16.8100, 22.9200, 19.6900,\n",
      "         19.6900, 16.8100, 19.6900, 19.6900, 19.6900, 19.6900, 19.6900, 22.9200,\n",
      "         19.6900, 19.6900, 19.6900, 22.9200, 19.6900, 16.8100, 19.6900, 19.6900,\n",
      "         19.6900, 22.9200, 19.6900, 19.6900, 19.6900, 19.6900, 19.6900, 19.6900,\n",
      "         22.9200, 19.6900, 19.6900, 19.6900, 16.8100, 19.6900, 19.6900, 19.6900,\n",
      "         19.6900, 19.6900, 19.6900, 19.6900, 22.9200, 16.8100, 22.9200, 19.6900,\n",
      "         19.6900, 19.6900, 19.6900, 19.6900, 19.6900, 19.6900, 19.6900, 22.9200,\n",
      "         23.4700, 19.6900, 16.8100, 22.9200]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.matmul(weights, torch.t(torch.round(predicted[:,:-(num_item + num_constraints)]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.,  5.,  7.,  9.,  7.,  5.,  3.,  7.,  4.,  8.],\n",
      "        [ 4.,  4.,  7.,  5.,  9., 10.,  5.,  6., 10.,  7.],\n",
      "        [ 9.,  8.,  5., 10.,  9.,  9., 12.,  9., 10., 10.],\n",
      "        [ 6.,  9.,  5.,  7.,  7.,  4.,  6.,  4.,  5.,  8.],\n",
      "        [11.,  8.,  3., 10.,  5.,  6.,  4.,  9.,  4.,  7.],\n",
      "        [ 5.,  9.,  5.,  6.,  4.,  5.,  6.,  9.,  3.,  5.],\n",
      "        [ 9.,  8.,  3.,  4.,  5.,  4.,  8.,  9.,  8.,  6.],\n",
      "        [ 8.,  7.,  6.,  6.,  5.,  6.,  7.,  5.,  8.,  8.],\n",
      "        [ 6.,  8.,  6.,  9.,  4.,  9.,  4.,  9.,  8.,  6.],\n",
      "        [ 6.,  4.,  9.,  6.,  4.,  9.,  6.,  6.,  6.,  6.],\n",
      "        [ 7., 10.,  3.,  5.,  8.,  7.,  4.,  5.,  6.,  6.],\n",
      "        [ 6.,  5.,  7.,  5.,  5.,  8.,  7.,  8.,  5.,  5.],\n",
      "        [ 6.,  9.,  7.,  5.,  5.,  6.,  4.,  6.,  5.,  5.],\n",
      "        [ 7.,  6.,  5.,  4.,  5.,  6.,  6.,  8.,  8.,  4.],\n",
      "        [ 7.,  7., 10., 10.,  7.,  6., 10.,  5.,  8.,  7.],\n",
      "        [ 5.,  6.,  8.,  7.,  7.,  6.,  5.,  9.,  7.,  8.],\n",
      "        [ 5.,  5.,  8.,  6.,  5.,  9.,  4.,  5.,  7.,  4.],\n",
      "        [ 8.,  4.,  4.,  4.,  5.,  8.,  9.,  4.,  5.,  7.],\n",
      "        [ 6.,  5.,  6.,  4.,  8.,  7.,  5.,  8.,  6.,  7.],\n",
      "        [ 4.,  8.,  5.,  5.,  7.,  3.,  5.,  4.,  6.,  4.],\n",
      "        [ 5.,  4.,  9.,  7.,  9.,  6.,  5., 10.,  7.,  7.],\n",
      "        [ 4.,  7.,  4.,  4.,  6.,  4.,  6.,  5.,  6.,  5.],\n",
      "        [ 4.,  8.,  7.,  6.,  4., 10.,  5., 10.,  7.,  4.],\n",
      "        [ 7.,  7.,  7.,  8.,  7.,  9.,  7.,  8.,  5.,  7.],\n",
      "        [ 6.,  7.,  5.,  4.,  5.,  7.,  5.,  8.,  8.,  6.],\n",
      "        [ 3.,  6.,  9.,  4.,  7.,  9.,  3.,  6.,  5.,  7.],\n",
      "        [ 3.,  5., 10.,  5.,  5.,  9.,  7.,  5.,  7.,  5.],\n",
      "        [ 6.,  7.,  7.,  4.,  8.,  6.,  5.,  7.,  6.,  8.],\n",
      "        [ 5.,  4.,  5.,  8.,  4.,  7.,  9.,  7.,  7.,  8.],\n",
      "        [ 6.,  5.,  7.,  5.,  8.,  7., 10.,  7.,  8.,  8.],\n",
      "        [ 8.,  6.,  7., 10.,  5.,  8.,  7.,  8., 10., 10.],\n",
      "        [ 8.,  6.,  6.,  6.,  6.,  7.,  4.,  6.,  5.,  7.],\n",
      "        [ 6.,  7.,  5.,  3.,  6.,  5., 10.,  4.,  4.,  5.],\n",
      "        [ 5.,  7.,  4.,  5.,  4.,  4.,  3.,  3.,  4.,  3.],\n",
      "        [ 6.,  4.,  7.,  6.,  4.,  7.,  5.,  4.,  7.,  5.],\n",
      "        [ 8.,  4.,  9.,  5.,  6.,  3.,  9.,  4.,  8.,  8.],\n",
      "        [ 8.,  8.,  6.,  5.,  6.,  8.,  4.,  7.,  3.,  5.],\n",
      "        [ 8.,  5.,  6.,  4.,  8.,  3.,  5.,  5.,  8.,  8.],\n",
      "        [ 5.,  4.,  7.,  6.,  7.,  7.,  5.,  7.,  6.,  6.],\n",
      "        [ 7.,  7.,  6.,  5.,  4.,  7.,  5.,  7.,  5.,  5.],\n",
      "        [ 5.,  9.,  7.,  5.,  3.,  5.,  4.,  8.,  8.,  9.],\n",
      "        [ 9.,  6., 10.,  7.,  4.,  8.,  9., 11.,  8.,  6.],\n",
      "        [ 5.,  6.,  7.,  9.,  5.,  5.,  5.,  9.,  8., 13.],\n",
      "        [ 4.,  7.,  8.,  3.,  4.,  4.,  5.,  4.,  6.,  3.],\n",
      "        [ 5.,  4.,  4.,  4.,  4.,  5.,  7.,  6.,  3.,  4.],\n",
      "        [ 6.,  8.,  7.,  9.,  5.,  4.,  8.,  7.,  8.,  8.],\n",
      "        [ 5.,  4.,  4.,  5.,  4.,  8.,  7.,  7.,  8.,  7.],\n",
      "        [10., 10.,  4.,  8.,  6.,  5.,  5.,  9., 11.,  8.],\n",
      "        [ 5.,  4., 10.,  6.,  9.,  9., 10.,  8., 10.,  7.],\n",
      "        [ 9.,  6.,  9.,  8.,  6.,  6.,  9.,  5.,  8.,  9.],\n",
      "        [ 7.,  5.,  6.,  4.,  6.,  8.,  7.,  4.,  5.,  4.],\n",
      "        [ 6.,  9.,  7.,  5.,  7.,  6.,  4.,  4.,  6.,  3.],\n",
      "        [ 3.,  6.,  7.,  3., 10.,  6.,  3.,  4.,  6.,  4.],\n",
      "        [ 7., 10.,  8.,  4.,  5.,  8.,  7.,  7.,  7.,  4.],\n",
      "        [ 4.,  5.,  3.,  3.,  7.,  8.,  4.,  6.,  3.,  6.],\n",
      "        [ 5.,  7.,  7.,  5.,  4.,  7.,  4.,  8.,  8.,  6.],\n",
      "        [ 4.,  6.,  6.,  4.,  5.,  6.,  3.,  5.,  5.,  3.],\n",
      "        [ 6.,  6.,  5.,  5.,  3.,  7.,  7.,  6.,  5.,  5.],\n",
      "        [ 5.,  4.,  6., 12.,  4.,  4.,  8.,  3.,  5.,  4.],\n",
      "        [ 4.,  8.,  8.,  4.,  8.,  7.,  4.,  3.,  7.,  6.],\n",
      "        [11., 10.,  4.,  8.,  7.,  7.,  4.,  9., 10.,  3.],\n",
      "        [ 5.,  4.,  5.,  6.,  7.,  9.,  4.,  5.,  7.,  3.],\n",
      "        [ 3.,  8.,  4.,  4.,  7.,  4.,  6.,  5.,  8.,  7.],\n",
      "        [ 8.,  6.,  8.,  7.,  4.,  7., 11.,  6.,  4.,  7.],\n",
      "        [ 6.,  8., 11.,  8.,  6.,  5.,  5.,  6.,  5.,  9.],\n",
      "        [ 4.,  6.,  5.,  4.,  8.,  6.,  6.,  5.,  6.,  4.],\n",
      "        [ 5.,  5., 10.,  5.,  4.,  6.,  5.,  9.,  8.,  7.],\n",
      "        [ 5.,  4.,  8.,  7.,  4.,  5.,  4.,  6.,  3.,  8.],\n",
      "        [ 6.,  6.,  4.,  4.,  4.,  9.,  5.,  4.,  6.,  4.],\n",
      "        [ 6.,  3.,  3.,  8.,  7.,  5.,  5.,  7.,  5.,  5.],\n",
      "        [ 5.,  2.,  9.,  4.,  9.,  8.,  4.,  4.,  5.,  5.],\n",
      "        [ 4.,  6.,  6.,  4.,  7.,  8.,  6.,  7.,  4.,  6.],\n",
      "        [ 7.,  8.,  9.,  6.,  4.,  6.,  8.,  6.,  6.,  6.],\n",
      "        [ 6.,  8.,  9.,  3.,  8.,  6.,  5.,  4.,  8.,  7.],\n",
      "        [ 9.,  5.,  3.,  7.,  4.,  6.,  5.,  6.,  7.,  9.],\n",
      "        [ 9.,  5.,  8., 11.,  7.,  7.,  9.,  4.,  8.,  9.],\n",
      "        [ 4.,  5., 10.,  5., 10.,  7.,  8.,  9.,  9., 10.],\n",
      "        [ 7.,  6.,  3.,  4.,  4.,  5.,  4.,  4.,  7.,  2.],\n",
      "        [ 8.,  7.,  4.,  6.,  8.,  8.,  8.,  5.,  5.,  7.],\n",
      "        [ 4.,  6.,  4.,  8.,  6.,  6.,  6.,  6.,  6.,  4.],\n",
      "        [ 6.,  5.,  5.,  6.,  6.,  3.,  7.,  6.,  3.,  7.],\n",
      "        [ 9., 10.,  4.,  7.,  3.,  4.,  8.,  9.,  4.,  9.],\n",
      "        [ 3.,  5.,  3.,  3.,  6.,  5.,  3.,  4.,  6.,  3.],\n",
      "        [ 7.,  5.,  6.,  5.,  3.,  7.,  8.,  8.,  5.,  6.],\n",
      "        [ 5.,  5.,  7.,  3.,  5.,  4.,  4.,  6.,  5.,  8.],\n",
      "        [ 5.,  6.,  9.,  8.,  5.,  5.,  9.,  6.,  3.,  5.],\n",
      "        [ 7.,  4.,  4.,  4.,  5.,  6.,  4.,  7.,  8.,  8.],\n",
      "        [ 5.,  7.,  7.,  4.,  7.,  8.,  4.,  4.,  4.,  5.],\n",
      "        [ 4.,  5.,  5.,  6.,  3.,  4.,  4.,  6.,  6.,  4.],\n",
      "        [ 6.,  4.,  6.,  7.,  9.,  7.,  4.,  4.,  3., 10.],\n",
      "        [ 7.,  7.,  4., 10.,  6.,  5.,  8.,  7.,  5.,  7.],\n",
      "        [ 8.,  7.,  6.,  4.,  6.,  3.,  6.,  4.,  6.,  4.],\n",
      "        [ 5.,  5.,  7.,  7.,  8.,  4.,  4.,  4.,  5.,  8.],\n",
      "        [ 8.,  6.,  6.,  4.,  7.,  8.,  4.,  5.,  6.,  4.],\n",
      "        [ 6.,  6.,  9.,  8.,  5.,  7.,  5., 10.,  8.,  5.],\n",
      "        [ 8.,  8.,  6.,  5.,  4.,  8.,  4.,  4.,  4.,  7.],\n",
      "        [ 6.,  9.,  7.,  6.,  4.,  9.,  5., 10.,  7.,  5.],\n",
      "        [ 7.,  7.,  4.,  7.,  4.,  6.,  7.,  6.,  6., 10.],\n",
      "        [ 7.,  4.,  6.,  9.,  8.,  8.,  7.,  6.,  8.,  8.],\n",
      "        [ 8.,  9.,  7., 10.,  7.,  4.,  5.,  6.,  7.,  7.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(w_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5900, 4.8700, 5.1900, 7.5900, 6.8000, 4.9700, 4.8400, 3.2200, 7.9000,\n",
      "         3.6200],\n",
      "        [3.1200, 6.8100, 7.0900, 3.2300, 4.4700, 5.2300, 6.1100, 5.2300, 4.2700,\n",
      "         6.6600]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(weights) # S matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 1., 0., 0., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 1.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 0.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# print(torch.round(predicted[:,:-12]) - torch.round(predicted[0,:-12]))\n",
    "print(torch.round((predicted[:,:-(num_constraints + num_item)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -1.,  1.,  1.,  0., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -1.,  0.,  1.,  1.,  0.,  0., -1.],\n",
      "        [ 0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0., -1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0., -1.],\n",
      "        [ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -1.,  0.,  1.,  1.,  0.,  0., -1.],\n",
      "        [ 0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0., -1.],\n",
      "        [ 0.,  0.,  0., -1.,  0.,  1.,  1.,  0.,  0., -1.],\n",
      "        [ 0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0., -1.],\n",
      "        [ 0.,  0.,  0., -1.,  0.,  1.,  1.,  0.,  0., -1.],\n",
      "        [-1.,  0.,  1.,  0.,  0.,  0.,  1., -1.,  0., -1.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(opt_sol[:15,:] - opt_sol[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2cbe3fc40>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0418, 0.1743, 0.7071, 0.7071, 0.7071, 0.7071, 0.7071, 0.7071, 0.7071,\n",
      "        0.7071, 1.0000, 1.0000], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(DYS_net.s_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(DYS_net.n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5900, 4.8700, 5.1900, 7.5900, 6.8000, 4.9700, 4.8400, 3.2200, 7.9000,\n",
      "         3.6200, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [3.1200, 6.8100, 7.0900, 3.2300, 4.4700, 5.2300, 6.1100, 5.2300, 4.2700,\n",
      "         6.6600, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0000]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(DYS_net.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.59 4.87 5.19 7.59 6.8  4.97 4.84 3.22 7.9  3.62]\n",
      " [3.12 6.81 7.09 3.23 4.47 5.23 6.11 5.23 4.27 6.66]]\n"
     ]
    }
   ],
   "source": [
    "print(weights_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7. 4. 6. 8. 9. 8. 8. 4. 7. 7.]\n"
     ]
    }
   ],
   "source": [
    "print(costs_numpy[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20., 20.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(DYS_net.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.5152, device='mps:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing the benchmark approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmodel = pyepo.model.grb.knapsackModel(weights_numpy, capacities_numpy)\n",
    "val_net = ValPredictNet(num_constraints, num_item, num_feat, device=device)\n",
    "val_net.to(device)\n",
    "max_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train a two-stage approach\n",
    "optimizer = optim.Adam(val_net.parameters(), lr=1e-2)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 test loss is  39.046939849853516\n",
      "epoch:  1 test loss is  37.78224182128906\n",
      "epoch:  2 test loss is  35.933258056640625\n",
      "epoch:  3 test loss is  33.123779296875\n",
      "epoch:  4 test loss is  29.031131744384766\n",
      "epoch:  5 test loss is  23.48961067199707\n",
      "epoch:  6 test loss is  16.995243072509766\n",
      "epoch:  7 test loss is  11.083303451538086\n",
      "epoch:  8 test loss is  7.683116912841797\n",
      "epoch:  9 test loss is  6.476044178009033\n",
      "epoch:  10 test loss is  5.358477592468262\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "while epoch <= max_epochs:\n",
    "    for d_batch, w_batch, opt_sol, opt_value in loader_train:\n",
    "        d_batch = d_batch.to(device)\n",
    "        w_batch = w_batch.to(device)\n",
    "        opt_sol = opt_sol.to(device)\n",
    "        opt_value = opt_value.to(device)\n",
    "        val_net.train()\n",
    "        optimizer.zero_grad()\n",
    "        w_predicted = val_net(d_batch)\n",
    "        loss = criterion(w_batch, w_predicted)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_ave = 0.95*train_loss_ave + 0.05*loss.item()\n",
    "        \n",
    "    val_net.eval()\n",
    "    test_loss = 0\n",
    "    for d_batch, w_batch, opt_sol, opt_value in loader_test:\n",
    "        d_batch = d_batch.to(device)\n",
    "        w_batch = w_batch.to(device)\n",
    "        opt_sol = opt_sol.to(device)\n",
    "        opt_value = opt_value.to(device)\n",
    "        w_predicted = val_net(d_batch)\n",
    "        test_loss += criterion(w_batch, w_predicted)\n",
    "    \n",
    "    # print(predicted)\n",
    "    scheduler.step(test_loss)\n",
    "    test_loss_hist.append(test_loss)\n",
    "    print('epoch: ', epoch, 'test loss is ', test_loss.item())\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value is  [31.]  while predicted value is  26.0\n",
      "Optimal value is  [30.]  while predicted value is  27.0\n",
      "Optimal value is  [39.]  while predicted value is  38.0\n",
      "Optimal value is  [26.]  while predicted value is  23.0\n",
      "Optimal value is  [37.]  while predicted value is  31.0\n",
      "Optimal value is  [27.]  while predicted value is  22.0\n",
      "Optimal value is  [32.]  while predicted value is  32.0\n",
      "Optimal value is  [29.]  while predicted value is  29.0\n",
      "Optimal value is  [30.]  while predicted value is  29.0\n",
      "Optimal value is  [27.]  while predicted value is  27.0\n",
      "Optimal value is  [30.]  while predicted value is  24.0\n",
      "Optimal value is  [29.]  while predicted value is  24.0\n",
      "Optimal value is  [26.]  while predicted value is  22.0\n",
      "Optimal value is  [27.]  while predicted value is  27.0\n",
      "Optimal value is  [30.]  while predicted value is  27.0\n",
      "Optimal value is  [29.]  while predicted value is  29.0\n",
      "Optimal value is  [24.]  while predicted value is  21.0\n",
      "Optimal value is  [29.]  while predicted value is  29.0\n",
      "Optimal value is  [29.]  while predicted value is  27.0\n",
      "Optimal value is  [23.]  while predicted value is  18.0\n",
      "Optimal value is  [33.]  while predicted value is  29.0\n",
      "Optimal value is  [22.]  while predicted value is  20.0\n",
      "Optimal value is  [29.]  while predicted value is  25.0\n",
      "Optimal value is  [31.]  while predicted value is  27.0\n",
      "Optimal value is  [28.]  while predicted value is  28.0\n",
      "Optimal value is  [26.]  while predicted value is  21.0\n",
      "Optimal value is  [26.]  while predicted value is  20.0\n",
      "Optimal value is  [29.]  while predicted value is  27.0\n",
      "Optimal value is  [28.]  while predicted value is  27.0\n",
      "Optimal value is  [31.]  while predicted value is  29.0\n",
      "Optimal value is  [36.]  while predicted value is  36.0\n",
      "Optimal value is  [28.]  while predicted value is  25.0\n",
      "Optimal value is  [26.]  while predicted value is  25.0\n",
      "Optimal value is  [19.]  while predicted value is  15.0\n",
      "Optimal value is  [22.]  while predicted value is  22.0\n",
      "Optimal value is  [28.]  while predicted value is  28.0\n",
      "Optimal value is  [29.]  while predicted value is  23.0\n",
      "Optimal value is  [29.]  while predicted value is  29.0\n",
      "Optimal value is  [26.]  while predicted value is  24.0\n",
      "Optimal value is  [26.]  while predicted value is  24.0\n",
      "Optimal value is  [30.]  while predicted value is  30.0\n",
      "Optimal value is  [37.]  while predicted value is  34.0\n",
      "Optimal value is  [36.]  while predicted value is  24.0\n",
      "Optimal value is  [21.]  while predicted value is  17.0\n",
      "Optimal value is  [23.]  while predicted value is  23.0\n",
      "Optimal value is  [30.]  while predicted value is  29.0\n",
      "Optimal value is  [27.]  while predicted value is  27.0\n",
      "Optimal value is  [38.]  while predicted value is  38.0\n",
      "Optimal value is  [32.]  while predicted value is  30.0\n",
      "Optimal value is  [31.]  while predicted value is  31.0\n",
      "Optimal value is  [26.]  while predicted value is  20.0\n",
      "Optimal value is  [26.]  while predicted value is  19.0\n",
      "Optimal value is  [24.]  while predicted value is  17.0\n",
      "Optimal value is  [29.]  while predicted value is  25.0\n",
      "Optimal value is  [25.]  while predicted value is  19.0\n",
      "Optimal value is  [27.]  while predicted value is  27.0\n",
      "Optimal value is  [20.]  while predicted value is  17.0\n",
      "Optimal value is  [26.]  while predicted value is  22.0\n",
      "Optimal value is  [26.]  while predicted value is  17.0\n",
      "Optimal value is  [25.]  while predicted value is  20.0\n",
      "Optimal value is  [37.]  while predicted value is  33.0\n",
      "Optimal value is  [26.]  while predicted value is  20.0\n",
      "Optimal value is  [23.]  while predicted value is  23.0\n",
      "Optimal value is  [32.]  while predicted value is  32.0\n",
      "Optimal value is  [29.]  while predicted value is  22.0\n",
      "Optimal value is  [23.]  while predicted value is  19.0\n",
      "Optimal value is  [29.]  while predicted value is  29.0\n",
      "Optimal value is  [26.]  while predicted value is  22.0\n",
      "Optimal value is  [24.]  while predicted value is  20.0\n",
      "Optimal value is  [26.]  while predicted value is  23.0\n",
      "Optimal value is  [27.]  while predicted value is  21.0\n",
      "Optimal value is  [26.]  while predicted value is  21.0\n",
      "Optimal value is  [27.]  while predicted value is  25.0\n",
      "Optimal value is  [27.]  while predicted value is  25.0\n",
      "Optimal value is  [31.]  while predicted value is  26.0\n",
      "Optimal value is  [33.]  while predicted value is  29.0\n",
      "Optimal value is  [33.]  while predicted value is  32.0\n",
      "Optimal value is  [21.]  while predicted value is  20.0\n",
      "Optimal value is  [31.]  while predicted value is  25.0\n",
      "Optimal value is  [22.]  while predicted value is  20.0\n",
      "Optimal value is  [25.]  while predicted value is  22.0\n",
      "Optimal value is  [34.]  while predicted value is  30.0\n",
      "Optimal value is  [18.]  while predicted value is  16.0\n",
      "Optimal value is  [30.]  while predicted value is  30.0\n",
      "Optimal value is  [24.]  while predicted value is  19.0\n",
      "Optimal value is  [26.]  while predicted value is  19.0\n",
      "Optimal value is  [30.]  while predicted value is  30.0\n",
      "Optimal value is  [25.]  while predicted value is  18.0\n",
      "Optimal value is  [20.]  while predicted value is  20.0\n",
      "Optimal value is  [32.]  while predicted value is  23.0\n",
      "Optimal value is  [31.]  while predicted value is  26.0\n",
      "Optimal value is  [25.]  while predicted value is  22.0\n",
      "Optimal value is  [25.]  while predicted value is  17.0\n",
      "Optimal value is  [28.]  while predicted value is  23.0\n",
      "Optimal value is  [30.]  while predicted value is  29.0\n",
      "Optimal value is  [27.]  while predicted value is  23.0\n",
      "Optimal value is  [30.]  while predicted value is  28.0\n",
      "Optimal value is  [30.]  while predicted value is  29.0\n",
      "Optimal value is  [31.]  while predicted value is  29.0\n",
      "Optimal value is  [31.]  while predicted value is  28.0\n"
     ]
    }
   ],
   "source": [
    "# compare solutions\n",
    "for d_batch, w_batch, opt_sol, opt_value in loader_test:\n",
    "    # load data\n",
    "    # convert to numpy\n",
    "    d_batch = d_batch.to(device)\n",
    "    # d_batch = d_batch.to(\"cpu\").detach().numpy()\n",
    "    w_batch = w_batch.to(\"cpu\").detach().numpy()\n",
    "    opt_sol = opt_sol.to(\"cpu\").detach().numpy()\n",
    "    opt_value = opt_value.to(\"cpu\").detach().numpy()\n",
    "    # predict\n",
    "    w_predicted = val_net(d_batch)\n",
    "    # print(w_predicted)\n",
    "    batch_size = d_batch.shape[0]\n",
    "    regret = 0\n",
    "    for i in range(batch_size):\n",
    "        optmodel.setObj(w_predicted[i])\n",
    "        pred_sol, pred_val = optmodel.solve()\n",
    "        regret += np.max(np.squeeze(opt_value[i]) - np.dot(w_batch[i], pred_sol),0)\n",
    "        # print((opt_value[i] - pred_val)/opt_value[i])\n",
    "        print('Optimal value is ', opt_value[i], ' while predicted value is ', np.dot(w_batch[i], pred_sol))\n",
    "#     if i == ind:\n",
    "#         # solve\n",
    "#         optmodel.setObj(cp[0])\n",
    "#         wp, _ = optmodel.solve()\n",
    "#         fig = plotSol(m, c, wp, weights, caps, \"Two-Stage with Linear Regression\")\n",
    "#         break\n",
    "\n",
    "    normalized_regret = regret/np.sum(opt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11071428571428571\n"
     ]
    }
   ],
   "source": [
    "print(normalized_regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
